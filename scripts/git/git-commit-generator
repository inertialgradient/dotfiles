#!/usr/bin/env python3

"""
git-commit-generator

Usage:
    git-commit-generator [--provider NAME] [--stream]

Dependencies:
    - OpenAI
    - pyxdg
"""

import argparse
import os
import subprocess
import sys
import tempfile
from pathlib import Path

import yaml
from openai import OpenAI
from xdg import BaseDirectory

CONFIG_DIR = Path(BaseDirectory.xdg_config_home)
DEFAULT_CONFIG = CONFIG_DIR / "git-commit-generator/config.yaml"


def fatal(msg):
    print(f"Error: {msg}", file=sys.stderr)
    sys.exit(1)


def load_config(path: Path):
    if not path.exists():
        fatal(f"Config file not found: {path}")
    try:
        with open(path, "r") as f:
            return yaml.safe_load(f)
    except Exception as e:
        fatal(f"Failed to parse YAML config: {e}")


def read_staged_diff():
    try:
        diff = subprocess.check_output(["git", "diff", "--cached"], text=True)
    except subprocess.CalledProcessError as e:
        fatal(f"git diff failed: {e}")

    if not diff.strip():
        fatal("No staged changes.")
    return diff


def generate_prompt(diff: str):
    return f"""Generate a Git commit message for the following staged diff.

Rules:
- Subject line ≤50 characters.
- Blank line after subject.
- Short descriptive body (1–4 sentences).
- Prefer itemized lists in the body.
- No chain-of-thought or explanation.
- Output ONLY the commit message.

Diff:
{diff}
"""


def create_client(base_url: str, api_key: str):
    return OpenAI(
        api_key=api_key,
        base_url=base_url.rstrip("/"),
    )


# -------------------------
# NON-STREAM ENGINES
# -------------------------


def call_responses_nonstream(client: OpenAI, model: str, prompt: str):
    try:
        resp = client.responses.create(
            model=model,
            input=prompt,
            temperature=0.2,
            max_output_tokens=200,
        )
    except Exception as e:
        fatal(f"API request failed: {e}")

    try:
        return resp.output[0].content[0].text
    except Exception:
        fatal(f"Model returned no commit message. Full response:\n{resp}")


def call_chat_nonstream(client: OpenAI, model: str, prompt: str):
    try:
        resp = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
        )
    except Exception as e:
        fatal(f"Chat API request failed: {e}")

    try:
        return resp.choices[0].message["content"]
    except Exception:
        fatal(f"Model returned no commit message. Full response:\n{resp}")


# -------------------------
# STREAM ENGINES
# -------------------------


def call_responses_stream(client: OpenAI, model: str, prompt: str):
    collected = []
    try:
        with client.responses.stream(
            model=model,
            input=prompt,
            temperature=0.2,
            max_output_tokens=200,
        ) as stream:
            for event in stream:
                if event.type == "response.output_text.delta":
                    text = event.delta or ""
                    collected.append(text)
                    print(text, end="", flush=True)

    except Exception as e:
        fatal(f"Streaming request failed: {e}")

    print("\n")
    return "".join(collected).strip()


def call_chat_stream(client: OpenAI, model: str, prompt: str):
    collected = []
    try:
        stream = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
            stream=True,
        )

        for chunk in stream:
            delta = chunk.choices[0].delta

            # delta is a ChoiceDelta object, not a dict
            text = getattr(delta, "content", None)
            if text:
                collected.append(text)
                print(text, end="", flush=True)

    except Exception as e:
        fatal(f"Chat streaming request failed: {e}")

    print("\n")
    return "".join(collected).strip()


# -------------------------
# UTILITIES
# -------------------------


def safe_input(prompt: str) -> str:
    try:
        return input(prompt)
    except KeyboardInterrupt:
        print("\nAborted.")
        sys.exit(1)


def edit_message(initial_msg: str):
    with tempfile.NamedTemporaryFile(delete=False, mode="w+", suffix=".txt") as tmp:
        tmp.write(initial_msg)
        tmp.flush()
        path = tmp.name

    editor = os.environ.get("EDITOR", "vi")
    os.system(f"{editor} {path}")

    with open(path, "r") as f:
        edited = f.read().strip()

    os.unlink(path)
    return edited


def commit_message(msg: str):
    try:
        subprocess.check_call(["git", "commit", "-m", msg])
    except subprocess.CalledProcessError:
        with open("commit-message", "w", encoding="utf8") as f:
            f.write(msg)
        fatal("Git commit failed.")


# -------------------------
# MAIN LOOP
# -------------------------


def main():
    parser = argparse.ArgumentParser(add_help=False)
    parser.add_argument("--provider", default=None)
    parser.add_argument("--stream", action="store_true", help="Enable streamed output")
    args, _ = parser.parse_known_args()

    config = load_config(DEFAULT_CONFIG)

    provider = (
        args.provider
        or os.environ.get("GIT_CG_PROVIDER")
        or config.get("default_provider")
    )
    if not provider:
        fatal("No provider specified (via --provider, env, or config).")

    providers = config.get("providers", {})
    if provider not in providers:
        fatal(f"Provider '{provider}' not found in config.")

    pconf = providers[provider]

    base_url = pconf.get("base_url")
    model = pconf.get("model")
    api_key_env = pconf.get("api_key_env")
    protocol = pconf.get("protocol", "responses").lower()

    if not base_url or not model or not api_key_env:
        fatal(f"Provider '{provider}' missing required fields.")

    api_key = os.environ.get(api_key_env)
    if not api_key:
        fatal(f"API key environment variable '{api_key_env}' not set.")

    if protocol not in {"responses", "chat"}:
        fatal(f"Invalid protocol '{protocol}' (must be 'responses' or 'chat').")

    client = create_client(base_url, api_key)

    diff = read_staged_diff()
    base_prompt = generate_prompt(diff)
    prompt = base_prompt

    while True:
        # -------------------------
        # Call appropriate engine
        # -------------------------
        if args.stream:
            if protocol == "responses":
                msg = call_responses_stream(client, model, prompt)
            else:
                msg = call_chat_stream(client, model, prompt)
        else:
            if protocol == "responses":
                msg = call_responses_nonstream(client, model, prompt)
            else:
                msg = call_chat_nonstream(client, model, prompt)
            print(msg)

        print("------------------------------------------------------------\n")

        ans = safe_input("Use, edit, regenerate, or abort? [y/e/r/N] ").strip().lower()

        if ans == "y":
            commit_message(msg)
            return

        elif ans == "e":
            edited = edit_message(msg)
            if not edited:
                fatal("Edited message is empty.")
            commit_message(edited)
            return

        elif ans == "r":
            fb = safe_input("Enter feedback for regeneration: ").strip()
            prompt = (
                f"{base_prompt}\n\nAdditional user feedback:\n{fb}\n\n"
                f"Regenerate the commit message following all prior rules."
            )

        else:
            print("Aborted.")
            sys.exit(1)


if __name__ == "__main__":
    main()
